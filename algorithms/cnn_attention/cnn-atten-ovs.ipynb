{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":861496,"sourceType":"datasetVersion","datasetId":457093}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.optimizers import Adam, SGD\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Mild_OVS = glob.glob('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/MildDemented/*.*')\nModerate_OVS = glob.glob('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/ModerateDemented/*.*')\nNonDemented_OVS = glob.glob('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/NonDemented/*.*')\nVeryMild_OVS = glob.glob('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/train/VeryMildDemented/*.*')\n\n\ndata_OVS=[]\nlabels_OVS = []\n\nfor i in NonDemented_OVS:   \n    image=tf.keras.preprocessing.image.load_img(i, target_size= (224,224))\n    image=np.array(image)\n    data_OVS.append(image)\n    labels_OVS.append(0)\n\nfor i in VeryMild_OVS:   \n    image=tf.keras.preprocessing.image.load_img(i, target_size= (224,224))\n    image=np.array(image)\n    data_OVS.append(image)\n    labels_OVS.append(1)\n\nfor i in Mild_OVS:   \n    image=tf.keras.preprocessing.image.load_img(i, target_size= (224,224))\n    image=np.array(image)\n    data_OVS.append(image)\n    labels_OVS.append(2)\n\nfor i in Moderate_OVS:   \n    image=tf.keras.preprocessing.image.load_img(i, target_size= (224,224))\n    image=np.array(image)\n    data_OVS.append(image)\n    labels_OVS.append(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_OVS = np.array(data_OVS)\nlabels_OVS = np.array(labels_OVS)\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\ndata_OVS,labels_OVS = sm.fit_resample(data_OVS.reshape(-1, 224 * 224 * 3), labels_OVS)\n\ndata_OVS = data_OVS.reshape(-1, 224, 224, 3)\n\nprint(data_OVS.shape, labels_OVS.shape)\nprint(len(data_OVS))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nMild_test = glob.glob('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/MildDemented/*.*')\nModerate_test = glob.glob('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/ModerateDemented/*.*')\nNonDemented_test = glob.glob('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/NonDemented/*.*')\nVeryMild_test = glob.glob('/kaggle/input/alzheimers-dataset-4-class-of-images/Alzheimer_s Dataset/test/VeryMildDemented/*.*')\n\n\ndata_test = []\nlabels_test  = []\n\nfor i in NonDemented_test:   \n    image=tf.keras.preprocessing.image.load_img(i, target_size= (224,224),grayscale=False)\n    image=np.array(image)\n    data_test.append(image)\n    labels_test.append(0)\n\nfor i in VeryMild_test:   \n    image=tf.keras.preprocessing.image.load_img(i, target_size= (224,224),grayscale=False)\n    image=np.array(image)\n    data_test.append(image)\n    labels_test.append(1)\n\nfor i in Mild_test:   \n    image=tf.keras.preprocessing.image.load_img(i, target_size= (224,224),grayscale=False)\n    image=np.array(image)\n    data_test.append(image)\n    labels_test.append(2)\n\nfor i in Moderate_test:   \n    image=tf.keras.preprocessing.image.load_img(i, target_size= (224,224),grayscale=False)\n    image=np.array(image)\n    data_test.append(image)\n    labels_test.append(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = np.array(data_test)\nlabels_test = np.array(labels_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.decomposition import PCA\n\n# Assuming you have a dataset of images in X (each row is an image)\n\n# Reshape each image to a 1D vector\n#n_samples, height, width, channels = data_OVS.shape\n#X_reshaped = data_OVS.reshape(n_samples, -1)  # -1 infers the size to maintain the number of elements\n#X_pca.reshape(-1, height, width, channels)\n\n\n# Perform PCA\n#n_components = 64  # You can adjust this value based on your needs\n#pca = PCA(n_components=n_components)\n#X_pca = pca.fit_transform(X_reshaped)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_pca.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.decomposition import PCA\n\n# Assuming you have a dataset of images in X (each row is an image)\n\n# Reshape each image to a 1D vector\n#n_samples, height, width, channels = data_OVS.shape\n#X_reshaped = data_OVS.reshape(n_samples, -1)  # -1 infers the size to maintain the number of elements\n\n\n#Perform PCA\n#n_components = 64  # You can adjust this value based on your needs\n#pca = PCA(n_components=n_components)\n#X_pca = pca.fit_transform(X_reshaped)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_pca.reshape(-1, height, width, channels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_reshaped = data_OVS.reshape(10240, height, width, channels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(rescale=1./255,\n                            rotation_range=45,\n                            width_shift_range=.15,\n                            height_shift_range=.15,\n                            horizontal_flip=False)\n\ntrain_data_gen = datagen.flow(\n    x=data_OVS,\n    y=labels_OVS,\n    batch_size=64,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_gen = datagen.flow(\n    x=data_test,\n    y=labels_test,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Attention","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = Sequential([\n    #Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n    #MaxPooling2D((2, 2)),\n    #Conv2D(64, (3, 3), activation='relu'),\n    #MaxPooling2D((2, 2)),\n    #Conv2D(128, (3, 3), activation='relu'),\n    #MaxPooling2D((2, 2)),\n    #Flatten(),\n   # Dense(128, activation='relu'),\n  #  Dropout(0.5),\n #   Dense(4, activation='softmax')\n#])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Layer\n\nclass SelfAttention(Layer):\n    def __init__(self):\n        super(SelfAttention, self).__init__()\n\n    def build(self, input_shape):\n        self.W_q = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n                                  initializer='uniform',\n                                  trainable=True)\n        self.W_k = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n                                  initializer='uniform',\n                                  trainable=True)\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, x):\n        Q = tf.matmul(x, self.W_q)\n        K = tf.matmul(x, self.W_k)\n        V = x\n        attention_weights = tf.nn.softmax(tf.matmul(Q, K, transpose_b=True), axis=-1)\n        output = tf.matmul(attention_weights, V)\n        return output\n\n# Create the model with the Self-Attention Layer\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(256, (3, 3), activation='relu'),  # Adding an additional convolutional layer\n    MaxPooling2D((2, 2)),\n    #SelfAttention(),\n    Flatten(),\n    Dense(256, activation='relu'),  # Increasing the number of units in the fully connected layer\n    Dropout(0.5),\n    Dense(128, activation='relu'),  # Adding another fully connected layer\n    Dropout(0.5),\n    Dense(4, activation='softmax')\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),  \n      tf.keras.metrics.AUC(name='auc')\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(train_data_gen, batch_size=32, epochs=10, validation_data=test_data_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}